{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8863a472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 3128\n",
      "Symbol                    object\n",
      "Date              datetime64[ns]\n",
      "Open                     float64\n",
      "High                     float64\n",
      "Low                      float64\n",
      "Close                    float64\n",
      "Percent_Change           float64\n",
      "Volume                   float64\n",
      "Turnover                 float64\n",
      "sentiment                float64\n",
      "dtype: object\n",
      "  Symbol       Date   Open   High    Low  Close  Percent_Change  Volume  \\\n",
      "0  NABIL 2012-01-01  873.0  879.0  851.0  879.0            0.69   799.0   \n",
      "1  NABIL 2012-01-02  879.0  913.0  882.0  900.0            2.39  1002.0   \n",
      "2  NABIL 2012-01-03  900.0  916.0  892.0  892.0           -0.89  2653.0   \n",
      "3  NABIL 2012-01-04  892.0  875.0  842.0  865.0           -3.03   490.0   \n",
      "4  NABIL 2012-01-05  865.0  875.0  858.0  858.0           -0.81   121.0   \n",
      "\n",
      "    Turnover  sentiment  \n",
      "0   684720.0        0.0  \n",
      "1   897160.0        0.0  \n",
      "2  2396445.0        0.0  \n",
      "3   417289.0        0.0  \n",
      "4   104175.0        0.0  \n",
      "\n",
      "--- Basic EDA (raw columns present) ---\n",
      "Date range: 2012-01-01 00:00:00 -> 2025-09-18 00:00:00\n",
      "Columns available for EDA: ['Open', 'High', 'Low', 'Close', 'Volume', 'Turnover', 'sentiment', 'Percent_Change']\n",
      "Null counts:\n",
      " Open              0\n",
      "High              0\n",
      "Low               0\n",
      "Close             0\n",
      "Volume            0\n",
      "Turnover          0\n",
      "sentiment         0\n",
      "Percent_Change    1\n",
      "dtype: int64\n",
      "Columns required/present after engineering: ['Open', 'High', 'Low', 'Close', 'Volume', 'Turnover', 'sentiment', 'Percent_Change', 'MA5', 'MA10', 'Return', 'RSI14', 'SMA20', 'SMA100', 'EMA20', 'BB_upper', 'BB_lower', 'MACD', 'MACD_signal', 'MACD_hist', 'STOCH_K', 'STOCH_D', 'Volume_MA20']\n",
      "Rows after feature engineering: 3127\n",
      "Final FEATURES used: ['Open', 'High', 'Low', 'Close', 'Volume', 'Turnover', 'Percent_Change', 'sentiment', 'MA5', 'MA10', 'Return', 'RSI14', 'SMA20', 'SMA100', 'EMA20', 'BB_upper', 'BB_lower', 'MACD', 'MACD_signal', 'MACD_hist', 'STOCH_K', 'STOCH_D', 'Volume_MA20']\n",
      "Total rows: 3127\n",
      "Test region: rows 2814 -> 3126 (last 313 rows, ~10.00%)\n",
      "Train+Val region: rows 0 -> 2813 (first 2814 rows, ~90.00%)\n",
      " -> Within that: Train rows 0 -> 2532 (count 2533); Val rows 2533 -> 2813 (count 281)\n",
      "Approx split (train/val/test): 2533/281/313 rows\n",
      "Total sequences (samples): (3067, 60, 23) (3067,)\n",
      "After splitting to sequences:\n",
      "  Train samples: (2473, 60, 23) (2473,)\n",
      "  Val samples:   (281, 60, 23) (281,)\n",
      "  Test samples:  (313, 60, 23) (313,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m22,528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,977</span> (136.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,977\u001b[0m (136.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,977</span> (136.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,977\u001b[0m (136.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.04715, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 13s - 167ms/step - loss: 0.0405 - val_loss: 0.0471 - learning_rate: 1.0000e-03\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.04715 to 0.03045, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0331 - val_loss: 0.0304 - learning_rate: 1.0000e-03\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.03045 to 0.01573, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0219 - val_loss: 0.0157 - learning_rate: 1.0000e-03\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.01573 to 0.01257, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 63ms/step - loss: 0.0183 - val_loss: 0.0126 - learning_rate: 1.0000e-03\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.01257\n",
      "78/78 - 5s - 61ms/step - loss: 0.0173 - val_loss: 0.0141 - learning_rate: 1.0000e-03\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.01257\n",
      "78/78 - 5s - 61ms/step - loss: 0.0163 - val_loss: 0.0130 - learning_rate: 1.0000e-03\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.01257 to 0.01137, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0154 - val_loss: 0.0114 - learning_rate: 1.0000e-03\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.01137\n",
      "78/78 - 5s - 61ms/step - loss: 0.0147 - val_loss: 0.0115 - learning_rate: 1.0000e-03\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss improved from 0.01137 to 0.01002, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 63ms/step - loss: 0.0141 - val_loss: 0.0100 - learning_rate: 1.0000e-03\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss improved from 0.01002 to 0.00795, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0122 - val_loss: 0.0079 - learning_rate: 1.0000e-03\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.00795\n",
      "78/78 - 5s - 61ms/step - loss: 0.0123 - val_loss: 0.0091 - learning_rate: 1.0000e-03\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss improved from 0.00795 to 0.00770, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 63ms/step - loss: 0.0115 - val_loss: 0.0077 - learning_rate: 1.0000e-03\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.00770\n",
      "78/78 - 5s - 62ms/step - loss: 0.0115 - val_loss: 0.0084 - learning_rate: 1.0000e-03\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss improved from 0.00770 to 0.00732, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0113 - val_loss: 0.0073 - learning_rate: 1.0000e-03\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss improved from 0.00732 to 0.00585, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 63ms/step - loss: 0.0102 - val_loss: 0.0058 - learning_rate: 1.0000e-03\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00585\n",
      "78/78 - 5s - 61ms/step - loss: 0.0097 - val_loss: 0.0058 - learning_rate: 1.0000e-03\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00585\n",
      "78/78 - 5s - 61ms/step - loss: 0.0097 - val_loss: 0.0061 - learning_rate: 1.0000e-03\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss improved from 0.00585 to 0.00505, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0091 - val_loss: 0.0050 - learning_rate: 1.0000e-03\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00505\n",
      "78/78 - 5s - 61ms/step - loss: 0.0090 - val_loss: 0.0058 - learning_rate: 1.0000e-03\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss improved from 0.00505 to 0.00469, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0087 - val_loss: 0.0047 - learning_rate: 1.0000e-03\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00469\n",
      "78/78 - 5s - 62ms/step - loss: 0.0083 - val_loss: 0.0048 - learning_rate: 1.0000e-03\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00469\n",
      "78/78 - 5s - 62ms/step - loss: 0.0082 - val_loss: 0.0052 - learning_rate: 1.0000e-03\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00469\n",
      "78/78 - 5s - 62ms/step - loss: 0.0079 - val_loss: 0.0050 - learning_rate: 1.0000e-03\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss improved from 0.00469 to 0.00388, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0076 - val_loss: 0.0039 - learning_rate: 1.0000e-03\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00388\n",
      "78/78 - 5s - 61ms/step - loss: 0.0070 - val_loss: 0.0042 - learning_rate: 1.0000e-03\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00388\n",
      "78/78 - 5s - 61ms/step - loss: 0.0074 - val_loss: 0.0046 - learning_rate: 1.0000e-03\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00388\n",
      "78/78 - 5s - 61ms/step - loss: 0.0069 - val_loss: 0.0044 - learning_rate: 1.0000e-03\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00388\n",
      "78/78 - 5s - 61ms/step - loss: 0.0068 - val_loss: 0.0043 - learning_rate: 1.0000e-03\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss improved from 0.00388 to 0.00384, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 63ms/step - loss: 0.0066 - val_loss: 0.0038 - learning_rate: 1.0000e-03\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00384\n",
      "78/78 - 5s - 61ms/step - loss: 0.0062 - val_loss: 0.0039 - learning_rate: 1.0000e-03\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss improved from 0.00384 to 0.00215, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0064 - val_loss: 0.0022 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss improved from 0.00215 to 0.00215, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 63ms/step - loss: 0.0064 - val_loss: 0.0022 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: val_loss improved from 0.00215 to 0.00210, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0063 - val_loss: 0.0021 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00210\n",
      "78/78 - 5s - 62ms/step - loss: 0.0062 - val_loss: 0.0021 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00210\n",
      "78/78 - 5s - 62ms/step - loss: 0.0063 - val_loss: 0.0023 - learning_rate: 5.0000e-04\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00210\n",
      "78/78 - 5s - 61ms/step - loss: 0.0065 - val_loss: 0.0022 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 37: val_loss improved from 0.00210 to 0.00208, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0065 - val_loss: 0.0021 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00208\n",
      "78/78 - 5s - 61ms/step - loss: 0.0080 - val_loss: 0.0031 - learning_rate: 2.5000e-04\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00208\n",
      "78/78 - 5s - 61ms/step - loss: 0.0062 - val_loss: 0.0024 - learning_rate: 2.5000e-04\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00208\n",
      "78/78 - 5s - 62ms/step - loss: 0.0056 - val_loss: 0.0021 - learning_rate: 2.5000e-04\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 41: val_loss improved from 0.00208 to 0.00207, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0052 - val_loss: 0.0021 - learning_rate: 2.5000e-04\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 42: val_loss improved from 0.00207 to 0.00190, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0050 - val_loss: 0.0019 - learning_rate: 2.5000e-04\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 43: val_loss improved from 0.00190 to 0.00181, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 63ms/step - loss: 0.0049 - val_loss: 0.0018 - learning_rate: 2.5000e-04\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 44: val_loss improved from 0.00181 to 0.00179, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0049 - val_loss: 0.0018 - learning_rate: 2.5000e-04\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 45: val_loss improved from 0.00179 to 0.00175, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0046 - val_loss: 0.0017 - learning_rate: 2.5000e-04\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 46: val_loss improved from 0.00175 to 0.00174, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0045 - val_loss: 0.0017 - learning_rate: 2.5000e-04\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 47: val_loss improved from 0.00174 to 0.00171, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 63ms/step - loss: 0.0046 - val_loss: 0.0017 - learning_rate: 2.5000e-04\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00171\n",
      "78/78 - 5s - 62ms/step - loss: 0.0045 - val_loss: 0.0017 - learning_rate: 2.5000e-04\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 49: val_loss improved from 0.00171 to 0.00171, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 63ms/step - loss: 0.0044 - val_loss: 0.0017 - learning_rate: 2.5000e-04\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 50: val_loss improved from 0.00171 to 0.00170, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 63ms/step - loss: 0.0045 - val_loss: 0.0017 - learning_rate: 2.5000e-04\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.00170\n",
      "78/78 - 5s - 61ms/step - loss: 0.0048 - val_loss: 0.0018 - learning_rate: 1.2500e-04\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.00170\n",
      "78/78 - 5s - 62ms/step - loss: 0.0042 - val_loss: 0.0018 - learning_rate: 1.2500e-04\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 53: val_loss improved from 0.00170 to 0.00168, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 63ms/step - loss: 0.0042 - val_loss: 0.0017 - learning_rate: 1.2500e-04\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 54: val_loss improved from 0.00168 to 0.00162, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 64ms/step - loss: 0.0040 - val_loss: 0.0016 - learning_rate: 1.2500e-04\n",
      "Epoch 55/100\n",
      "\n",
      "Epoch 55: val_loss improved from 0.00162 to 0.00161, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 70ms/step - loss: 0.0040 - val_loss: 0.0016 - learning_rate: 1.2500e-04\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00161\n",
      "78/78 - 5s - 68ms/step - loss: 0.0042 - val_loss: 0.0016 - learning_rate: 1.2500e-04\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 57: val_loss improved from 0.00161 to 0.00158, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 70ms/step - loss: 0.0039 - val_loss: 0.0016 - learning_rate: 1.2500e-04\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 58: val_loss improved from 0.00158 to 0.00157, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 70ms/step - loss: 0.0040 - val_loss: 0.0016 - learning_rate: 1.2500e-04\n",
      "Epoch 59/100\n",
      "\n",
      "Epoch 59: val_loss improved from 0.00157 to 0.00156, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 70ms/step - loss: 0.0041 - val_loss: 0.0016 - learning_rate: 1.2500e-04\n",
      "Epoch 60/100\n",
      "\n",
      "Epoch 60: val_loss improved from 0.00156 to 0.00156, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 69ms/step - loss: 0.0037 - val_loss: 0.0016 - learning_rate: 1.2500e-04\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.00156\n",
      "78/78 - 5s - 68ms/step - loss: 0.0040 - val_loss: 0.0016 - learning_rate: 1.2500e-04\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 62: val_loss improved from 0.00156 to 0.00155, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 70ms/step - loss: 0.0038 - val_loss: 0.0016 - learning_rate: 1.2500e-04\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.00155\n",
      "78/78 - 5s - 70ms/step - loss: 0.0040 - val_loss: 0.0016 - learning_rate: 1.2500e-04\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 64: val_loss improved from 0.00155 to 0.00154, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 70ms/step - loss: 0.0038 - val_loss: 0.0015 - learning_rate: 1.2500e-04\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 65: val_loss improved from 0.00154 to 0.00152, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 70ms/step - loss: 0.0040 - val_loss: 0.0015 - learning_rate: 6.2500e-05\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 66: val_loss improved from 0.00152 to 0.00149, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 70ms/step - loss: 0.0038 - val_loss: 0.0015 - learning_rate: 6.2500e-05\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.00149\n",
      "78/78 - 5s - 68ms/step - loss: 0.0038 - val_loss: 0.0015 - learning_rate: 6.2500e-05\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 68: val_loss improved from 0.00149 to 0.00148, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 6s - 71ms/step - loss: 0.0037 - val_loss: 0.0015 - learning_rate: 6.2500e-05\n",
      "Epoch 69/100\n",
      "\n",
      "Epoch 69: val_loss improved from 0.00148 to 0.00147, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 69ms/step - loss: 0.0035 - val_loss: 0.0015 - learning_rate: 6.2500e-05\n",
      "Epoch 70/100\n",
      "\n",
      "Epoch 70: val_loss improved from 0.00147 to 0.00147, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 70ms/step - loss: 0.0037 - val_loss: 0.0015 - learning_rate: 6.2500e-05\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 71: val_loss improved from 0.00147 to 0.00147, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 6s - 71ms/step - loss: 0.0035 - val_loss: 0.0015 - learning_rate: 6.2500e-05\n",
      "Epoch 72/100\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.00147\n",
      "78/78 - 5s - 69ms/step - loss: 0.0037 - val_loss: 0.0015 - learning_rate: 6.2500e-05\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.00147\n",
      "78/78 - 5s - 68ms/step - loss: 0.0037 - val_loss: 0.0015 - learning_rate: 6.2500e-05\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.00147\n",
      "78/78 - 5s - 64ms/step - loss: 0.0036 - val_loss: 0.0015 - learning_rate: 6.2500e-05\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 75: val_loss improved from 0.00147 to 0.00146, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 63ms/step - loss: 0.0036 - val_loss: 0.0015 - learning_rate: 6.2500e-05\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 76: val_loss improved from 0.00146 to 0.00144, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0037 - val_loss: 0.0014 - learning_rate: 3.1250e-05\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 77: val_loss improved from 0.00144 to 0.00143, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0036 - val_loss: 0.0014 - learning_rate: 3.1250e-05\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.00143\n",
      "78/78 - 5s - 62ms/step - loss: 0.0038 - val_loss: 0.0014 - learning_rate: 3.1250e-05\n",
      "Epoch 79/100\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.00143\n",
      "78/78 - 5s - 61ms/step - loss: 0.0036 - val_loss: 0.0014 - learning_rate: 3.1250e-05\n",
      "Epoch 80/100\n",
      "\n",
      "Epoch 80: val_loss improved from 0.00143 to 0.00143, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0036 - val_loss: 0.0014 - learning_rate: 3.1250e-05\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.00143\n",
      "78/78 - 5s - 61ms/step - loss: 0.0035 - val_loss: 0.0014 - learning_rate: 3.1250e-05\n",
      "Epoch 82/100\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.00143\n",
      "78/78 - 5s - 62ms/step - loss: 0.0036 - val_loss: 0.0014 - learning_rate: 1.5625e-05\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.00143\n",
      "78/78 - 5s - 61ms/step - loss: 0.0036 - val_loss: 0.0014 - learning_rate: 1.5625e-05\n",
      "Epoch 84/100\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.00143\n",
      "78/78 - 5s - 61ms/step - loss: 0.0035 - val_loss: 0.0014 - learning_rate: 1.5625e-05\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 85: val_loss improved from 0.00143 to 0.00143, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0037 - val_loss: 0.0014 - learning_rate: 1.5625e-05\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 86: val_loss improved from 0.00143 to 0.00143, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 62ms/step - loss: 0.0035 - val_loss: 0.0014 - learning_rate: 1.5625e-05\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.00143\n",
      "78/78 - 7s - 89ms/step - loss: 0.0035 - val_loss: 0.0014 - learning_rate: 1.5625e-05\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.00143\n",
      "78/78 - 5s - 68ms/step - loss: 0.0036 - val_loss: 0.0014 - learning_rate: 7.8125e-06\n",
      "Epoch 89/100\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.00143\n",
      "78/78 - 5s - 70ms/step - loss: 0.0035 - val_loss: 0.0014 - learning_rate: 7.8125e-06\n",
      "Epoch 90/100\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.00143\n",
      "78/78 - 6s - 71ms/step - loss: 0.0034 - val_loss: 0.0014 - learning_rate: 7.8125e-06\n",
      "Epoch 91/100\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00143\n",
      "78/78 - 6s - 71ms/step - loss: 0.0034 - val_loss: 0.0014 - learning_rate: 7.8125e-06\n",
      "Epoch 92/100\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00143\n",
      "78/78 - 5s - 70ms/step - loss: 0.0034 - val_loss: 0.0014 - learning_rate: 7.8125e-06\n",
      "Epoch 93/100\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00143\n",
      "78/78 - 5s - 68ms/step - loss: 0.0036 - val_loss: 0.0014 - learning_rate: 7.8125e-06\n",
      "Epoch 94/100\n",
      "\n",
      "Epoch 94: val_loss improved from 0.00143 to 0.00142, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 6s - 71ms/step - loss: 0.0036 - val_loss: 0.0014 - learning_rate: 3.9063e-06\n",
      "Epoch 95/100\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00142\n",
      "78/78 - 5s - 70ms/step - loss: 0.0035 - val_loss: 0.0014 - learning_rate: 3.9063e-06\n",
      "Epoch 96/100\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00142\n",
      "78/78 - 5s - 70ms/step - loss: 0.0034 - val_loss: 0.0014 - learning_rate: 3.9063e-06\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 97: val_loss improved from 0.00142 to 0.00142, saving model to outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 5s - 69ms/step - loss: 0.0033 - val_loss: 0.0014 - learning_rate: 3.9063e-06\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00142\n",
      "78/78 - 5s - 70ms/step - loss: 0.0036 - val_loss: 0.0014 - learning_rate: 3.9063e-06\n",
      "Epoch 99/100\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.00142\n",
      "78/78 - 5s - 70ms/step - loss: 0.0035 - val_loss: 0.0014 - learning_rate: 3.9063e-06\n",
      "Epoch 100/100\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00142\n",
      "78/78 - 6s - 71ms/step - loss: 0.0035 - val_loss: 0.0014 - learning_rate: 1.9531e-06\n",
      "Restoring model weights from the end of the best epoch: 97.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 126ms/step\n",
      "Test MAE: 18.0065, RMSE: 22.9498\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Sentiment importance (RMSE increase % if shuffled): -0.59%\n",
      "Actual label distribution: {np.str_('DOWN'): np.int64(164), np.str_('UNCHANGED'): np.int64(32), np.str_('UP'): np.int64(117)}\n",
      "Predicted label distribution: {np.str_('DOWN'): np.int64(79), np.str_('UNCHANGED'): np.int64(5), np.str_('UP'): np.int64(229)}\n",
      "\n",
      "--- Direction classification metrics ---\n",
      "Accuracy: 0.4633\n",
      "Macro Precision: 0.4117, Macro Recall: 0.3807, Macro F1: 0.3355\n",
      "Weighted Precision: 0.5005, Weighted Recall: 0.4633, Weighted F1: 0.4221\n",
      "\n",
      "Per-class (labels = ['UP','DOWN','UNCHANGED']):\n",
      "  UP        -> Precision: 0.4148, Recall: 0.8120, F1: 0.5491\n",
      "  DOWN      -> Precision: 0.6203, Recall: 0.2988, F1: 0.4033\n",
      "  UNCHANGED -> Precision: 0.2000, Recall: 0.0312, F1: 0.0541\n",
      "\n",
      "Full classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          UP       0.41      0.81      0.55       117\n",
      "        DOWN       0.62      0.30      0.40       164\n",
      "   UNCHANGED       0.20      0.03      0.05        32\n",
      "\n",
      "    accuracy                           0.46       313\n",
      "   macro avg       0.41      0.38      0.34       313\n",
      "weighted avg       0.50      0.46      0.42       313\n",
      "\n",
      "Confusion matrix (rows=actual, cols=predicted):\n",
      "[[ 95  21   1]\n",
      " [112  49   3]\n",
      " [ 22   9   1]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Next day prediction ---\n",
      "Last data date: 2025-09-18, last close: 483.0000\n",
      "Predicted next-day close (approx): 522.9658\n",
      "Predicted movement: UP\n",
      "Saved: outputs\\NABIL\\NABIL_test_predictions.csv\n",
      "Saved: outputs\\NABIL\\NABIL_test_predictions_with_labels.csv\n",
      "Saved last sequence (scaled): outputs\\NABIL\\NABIL_last_sequence_features_scaled.csv\n",
      "Saved scaler to outputs\\NABIL\\NABIL_scaler.joblib\n",
      "Saved HDF5 model to: outputs\\NABIL\\NABIL_model.h5\n",
      "Warning: saving SavedModel failed: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=outputs\\NABIL\\NABIL_savedmodel_20251105T154306Z.\n",
      "Saved meta JSON: outputs\\NABIL\\NABIL_meta.json\n",
      "Wrote consolidated JSON: outputs\\NABIL\\NABIL.json\n",
      "Model checkpoint (best) at: outputs\\NABIL\\best_lstm_20251105T154306Z.h5\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "# fixed_improved_lstm_adbl_with_sentiment_and_techindicators_with_classification_fixed.py\n",
    "\"\"\"\n",
    "Pipeline with sentiment + technical indicators support, plus directional classification metrics.\n",
    "All outputs (JSON, PNGs, CSVs, models) are saved to outputs/<SYMBOL>/.\n",
    "Fixes broadcasting errors when inverse-transforming the scaled close values.\n",
    "Requirements: pandas, numpy, matplotlib, seaborn, scikit-learn, tensorflow, joblib\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# -------------------------\n",
    "# Config / reproducibility\n",
    "# -------------------------\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "DATA_PATH = \"NABIL.csv\"    # <-- change to your CSV file (or path)\n",
    "SEQ_LEN = 60\n",
    "\n",
    "TEST_PCT = 0.10\n",
    "VAL_WITHIN_TRAIN_PCT = 0.10\n",
    "\n",
    "RAW_FEATURES_REQUIRED = ['Open', 'High', 'Low', 'Close', 'Volume', 'Turnover', 'sentiment']\n",
    "PCT_CANDIDATES = ['Percent_Change', 'Percent Change', 'PctChange', 'Percent']\n",
    "\n",
    "ENGINEERED = [\n",
    "    'MA5', 'MA10', 'Return', 'RSI14',\n",
    "    'SMA20', 'SMA100', 'EMA20',\n",
    "    'BB_upper', 'BB_lower',\n",
    "    'MACD', 'MACD_signal', 'MACD_hist',\n",
    "    'STOCH_K', 'STOCH_D',\n",
    "    'Volume_MA20'\n",
    "]\n",
    "\n",
    "FEATURES = None\n",
    "CLOSE_COL_NAME = 'Close'\n",
    "\n",
    "MODEL_SAVE_DIR = \"saved_models\"\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# outputs directory (symbol-specific)\n",
    "# -------------------------\n",
    "symbol = os.path.splitext(os.path.basename(DATA_PATH))[0].upper()\n",
    "OUTPUTS_DIR = \"outputs\"\n",
    "symbol_dir = os.path.join(OUTPUTS_DIR, symbol)\n",
    "os.makedirs(symbol_dir, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Helper indicator functions\n",
    "# -------------------------\n",
    "def compute_RSI(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.rolling(window=period, min_periods=period).mean()\n",
    "    avg_loss = loss.rolling(window=period, min_periods=period).mean()\n",
    "    rs = avg_gain / (avg_loss + 1e-9)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi.fillna(50.0)\n",
    "\n",
    "def compute_bollinger(series, window=20, n_std=2):\n",
    "    sma = series.rolling(window=window, min_periods=1).mean()\n",
    "    std = series.rolling(window=window, min_periods=1).std().fillna(0)\n",
    "    upper = sma + n_std * std\n",
    "    lower = sma - n_std * std\n",
    "    return upper, lower\n",
    "\n",
    "def compute_macd(series, fast=12, slow=26, signal=9):\n",
    "    ema_fast = series.ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = series.ewm(span=slow, adjust=False).mean()\n",
    "    macd = ema_fast - ema_slow\n",
    "    macd_signal = macd.ewm(span=signal, adjust=False).mean()\n",
    "    macd_hist = macd - macd_signal\n",
    "    return macd, macd_signal, macd_hist\n",
    "\n",
    "def compute_stochastic(high, low, close, k_period=14, d_period=3):\n",
    "    low_min = low.rolling(window=k_period, min_periods=1).min()\n",
    "    high_max = high.rolling(window=k_period, min_periods=1).max()\n",
    "    denom = (high_max - low_min).replace(0, np.nan)\n",
    "    k = 100 * (close - low_min) / denom\n",
    "    k = k.fillna(50.0)\n",
    "    d = k.rolling(window=d_period, min_periods=1).mean().fillna(50.0)\n",
    "    return k, d\n",
    "\n",
    "# -------------------------\n",
    "# 1) Load & basic cleaning\n",
    "# -------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = [c.strip().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "# Unify percent column\n",
    "found_pct = None\n",
    "for cand in PCT_CANDIDATES:\n",
    "    col_name = cand.replace(\" \", \"_\")\n",
    "    if col_name in df.columns:\n",
    "        df['Percent_Change'] = df[col_name]\n",
    "        found_pct = col_name\n",
    "        break\n",
    "if found_pct is None:\n",
    "    df['Percent_Change'] = np.nan\n",
    "\n",
    "# Clean percent string and numeric columns\n",
    "df['Percent_Change'] = df['Percent_Change'].astype(str).str.replace('%', '', regex=False).str.replace(',', '', regex=False).str.strip()\n",
    "df['Percent_Change'] = pd.to_numeric(df['Percent_Change'], errors='coerce')\n",
    "\n",
    "# Clean raw numeric columns (remove commas/quotes)\n",
    "for c in RAW_FEATURES_REQUIRED:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(str).str.replace(',', '', regex=False).str.replace('\"', '', regex=False).str.strip()\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    else:\n",
    "        print(f\"WARNING: expected raw column '{c}' not found in CSV.\")\n",
    "\n",
    "# Date\n",
    "if 'Date' not in df.columns:\n",
    "    raise ValueError(\"CSV must contain a 'Date' column\")\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "print(\"Loaded rows:\", len(df))\n",
    "print(df.dtypes)\n",
    "print(df.head())\n",
    "\n",
    "# -------------------------\n",
    "# 2) EDA checks (quick plots)\n",
    "# -------------------------\n",
    "print(\"\\n--- Basic EDA (raw columns present) ---\")\n",
    "print(\"Date range:\", df['Date'].min(), \"->\", df['Date'].max())\n",
    "\n",
    "eda_cols = [c for c in (RAW_FEATURES_REQUIRED + ['Percent_Change']) if c in df.columns]\n",
    "print(\"Columns available for EDA:\", eda_cols)\n",
    "print(\"Null counts:\\n\", df[eda_cols].isna().sum())\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(df['Date'], df[CLOSE_COL_NAME], linewidth=1)\n",
    "plt.title(\"Close price over time\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Close\")\n",
    "plt.tight_layout()\n",
    "eda_path = os.path.join(symbol_dir, f\"{symbol}_close_over_time.png\")\n",
    "plt.savefig(eda_path); plt.close()\n",
    "\n",
    "num_cols_for_corr = [c for c in eda_cols if c in df.columns and df[c].dtype != object]\n",
    "if len(num_cols_for_corr) >= 3:\n",
    "    corr = df[num_cols_for_corr].corr()\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "    plt.title(\"Correlation (raw cols)\")\n",
    "    corr_path = os.path.join(symbol_dir, f\"{symbol}_corr.png\")\n",
    "    plt.tight_layout(); plt.savefig(corr_path); plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# 3) Feature engineering (technical indicators)\n",
    "# -------------------------\n",
    "if CLOSE_COL_NAME not in df.columns:\n",
    "    raise ValueError(f\"'{CLOSE_COL_NAME}' column is required but not present.\")\n",
    "\n",
    "df['MA5'] = df[CLOSE_COL_NAME].rolling(window=5, min_periods=1).mean()\n",
    "df['MA10'] = df[CLOSE_COL_NAME].rolling(window=10, min_periods=1).mean()\n",
    "df['Return'] = df[CLOSE_COL_NAME].pct_change().fillna(0)\n",
    "\n",
    "df['RSI14'] = compute_RSI(df[CLOSE_COL_NAME], 14)\n",
    "\n",
    "df['SMA20'] = df[CLOSE_COL_NAME].rolling(window=20, min_periods=1).mean()\n",
    "df['SMA100'] = df[CLOSE_COL_NAME].rolling(window=100, min_periods=1).mean()\n",
    "df['EMA20'] = df[CLOSE_COL_NAME].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "df['BB_upper'], df['BB_lower'] = compute_bollinger(df[CLOSE_COL_NAME], window=20, n_std=2)\n",
    "\n",
    "df['MACD'], df['MACD_signal'], df['MACD_hist'] = compute_macd(df[CLOSE_COL_NAME], fast=12, slow=26, signal=9)\n",
    "\n",
    "# if any of High/Low missing compute_stochastic will fail - guard ensures presence\n",
    "if all(c in df.columns for c in ['High','Low','Close']):\n",
    "    df['STOCH_K'], df['STOCH_D'] = compute_stochastic(df['High'], df['Low'], df['Close'], k_period=14, d_period=3)\n",
    "\n",
    "if 'Volume' in df.columns:\n",
    "    df['Volume_MA20'] = df['Volume'].rolling(window=20, min_periods=1).mean()\n",
    "\n",
    "required_after_engineer = RAW_FEATURES_REQUIRED + ['Percent_Change'] + ENGINEERED\n",
    "present_required = [c for c in required_after_engineer if c in df.columns]\n",
    "print(\"Columns required/present after engineering:\", present_required)\n",
    "df = df.dropna(subset=[c for c in present_required if c in df.columns]).reset_index(drop=True)\n",
    "print(\"Rows after feature engineering:\", len(df))\n",
    "\n",
    "FEATURES = [c for c in [\n",
    "    'Open','High','Low','Close','Volume','Turnover','Percent_Change','sentiment',\n",
    "    'MA5','MA10','Return','RSI14',\n",
    "    'SMA20','SMA100','EMA20','BB_upper','BB_lower',\n",
    "    'MACD','MACD_signal','MACD_hist','STOCH_K','STOCH_D','Volume_MA20'\n",
    "] if c in df.columns]\n",
    "\n",
    "if 'Close' not in FEATURES:\n",
    "    raise ValueError(\"'Close' must be in FEATURES after engineering.\")\n",
    "\n",
    "CLOSE_IDX = FEATURES.index('Close')\n",
    "print(\"Final FEATURES used:\", FEATURES)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Train / Val / Test splits (time-based)\n",
    "# -------------------------\n",
    "n = len(df)\n",
    "if n <= SEQ_LEN + 5:\n",
    "    raise ValueError(\"Dataset too small for SEQ_LEN and splitting. Provide more rows or reduce SEQ_LEN.\")\n",
    "\n",
    "test_start = int(np.floor(n * (1.0 - TEST_PCT)))\n",
    "train_and_val_end = test_start\n",
    "train_and_val_n = train_and_val_end\n",
    "\n",
    "val_within_train_n = int(np.floor(train_and_val_n * VAL_WITHIN_TRAIN_PCT))\n",
    "train_end = train_and_val_n - val_within_train_n\n",
    "val_start = train_end\n",
    "val_end = train_and_val_n\n",
    "\n",
    "print(f\"Total rows: {n}\")\n",
    "print(f\"Test region: rows {test_start} -> {n-1} (last {n-test_start} rows, ~{TEST_PCT*100:.2f}%)\")\n",
    "print(f\"Train+Val region: rows 0 -> {train_and_val_end-1} (first {train_and_val_end} rows, ~{(1-TEST_PCT)*100:.2f}%)\")\n",
    "print(f\" -> Within that: Train rows 0 -> {train_end-1} (count {train_end}); Val rows {val_start} -> {val_end-1} (count {val_within_train_n})\")\n",
    "print(f\"Approx split (train/val/test): {train_end}/{val_within_train_n}/{n-test_start} rows\")\n",
    "\n",
    "raw_data = df[FEATURES].values.astype(float)\n",
    "\n",
    "if train_end <= 0:\n",
    "    raise ValueError(\"Training region computed as empty. Reduce VAL_WITHIN_TRAIN_PCT or TEST_PCT.\")\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(raw_data[:train_end])  # fit only on training region (first train_end rows)\n",
    "\n",
    "scaled_all = scaler.transform(raw_data)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Sequence creation\n",
    "# -------------------------\n",
    "def create_sequences_from_scaled(scaled, seq_len, close_idx):\n",
    "    X, y, seq_idx = [], [], []\n",
    "    nrows = len(scaled)\n",
    "    for i in range(seq_len, nrows):\n",
    "        X.append(scaled[i-seq_len:i])\n",
    "        y.append(scaled[i, close_idx])\n",
    "        seq_idx.append(i)\n",
    "    return np.array(X), np.array(y), np.array(seq_idx)\n",
    "\n",
    "X_all, y_all, seq_idx = create_sequences_from_scaled(scaled_all, SEQ_LEN, CLOSE_IDX)\n",
    "print(\"Total sequences (samples):\", X_all.shape, y_all.shape)\n",
    "\n",
    "train_mask = seq_idx < train_end\n",
    "val_mask = (seq_idx >= val_start) & (seq_idx < val_end)\n",
    "test_mask = seq_idx >= test_start\n",
    "\n",
    "X_train, y_train = X_all[train_mask], y_all[train_mask]\n",
    "X_val, y_val = X_all[val_mask], y_all[val_mask]\n",
    "X_test, y_test = X_all[test_mask], y_all[test_mask]\n",
    "\n",
    "print(\"After splitting to sequences:\")\n",
    "print(\"  Train samples:\", X_train.shape, y_train.shape)\n",
    "print(\"  Val samples:  \", X_val.shape, y_val.shape)\n",
    "print(\"  Test samples: \", X_test.shape, y_test.shape)\n",
    "\n",
    "if len(X_train) == 0:\n",
    "    raise ValueError(\"Training set is empty after splitting — reduce VAL_WITHIN_TRAIN_PCT or TEST_PCT, or add more data.\")\n",
    "if len(X_test) == 0:\n",
    "    raise ValueError(\"Test set is empty — increase dataset size or reduce TEST_PCT.\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Build the LSTM model (regularized)\n",
    "# -------------------------\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, len(FEATURES)),\n",
    "         recurrent_dropout=0.15, kernel_regularizer=regularizers.l2(1e-4)),\n",
    "    Dropout(0.25),\n",
    "    LSTM(32, recurrent_dropout=0.15, kernel_regularizer=regularizers.l2(1e-4)),\n",
    "    Dropout(0.25),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.summary()\n",
    "\n",
    "# Callbacks + model paths\n",
    "timestamp = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "ckpt_path = os.path.join(symbol_dir, f\"best_lstm_{timestamp}.h5\")\n",
    "symbol_model_h5 = os.path.join(symbol_dir, f\"{symbol}_model.h5\")\n",
    "symbol_savedmodel_dir = os.path.join(symbol_dir, f\"{symbol}_savedmodel_{timestamp}\")\n",
    "symbol_scaler_path = os.path.join(symbol_dir, f\"{symbol}_scaler.joblib\")\n",
    "symbol_pred_csv = os.path.join(symbol_dir, f\"{symbol}_test_predictions.csv\")\n",
    "symbol_pred_labels_csv = os.path.join(symbol_dir, f\"{symbol}_test_predictions_with_labels.csv\")\n",
    "symbol_last_seq_csv = os.path.join(symbol_dir, f\"{symbol}_last_sequence_features_scaled.csv\")\n",
    "symbol_meta_json = os.path.join(symbol_dir, f\"{symbol}_meta.json\")\n",
    "loss_plot_path = os.path.join(symbol_dir, f\"{symbol}_train_loss.png\")\n",
    "pred_plot_path = os.path.join(symbol_dir, f\"{symbol}_prediction.png\")\n",
    "confusion_path = os.path.join(symbol_dir, f\"{symbol}_confusion.png\")\n",
    "eda_path = os.path.join(symbol_dir, f\"{symbol}_close_over_time.png\")\n",
    "corr_path = os.path.join(symbol_dir, f\"{symbol}_corr.png\")\n",
    "json_path = os.path.join(symbol_dir, f\"{symbol}.json\")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=12, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, verbose=1, min_lr=1e-6),\n",
    "    ModelCheckpoint(ckpt_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# 7) Train (no shuffle)\n",
    "# -------------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    shuffle=False,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# plot losses and save\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss (MSE, log scale)')\n",
    "plt.legend(); plt.title('Training vs Validation Loss')\n",
    "plt.tight_layout(); plt.savefig(loss_plot_path); plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# 8) Test evaluation and robust inverse transform helper\n",
    "# -------------------------\n",
    "def inverse_transform_close(scaled_close_array, scaler, close_idx=CLOSE_IDX):\n",
    "    \"\"\"\n",
    "    scaled_close_array: 1D array-like of scaled close values (same scale as scaler).\n",
    "    scaler: fitted MinMaxScaler\n",
    "    close_idx: index of the close feature in the scaler input.\n",
    "    Returns: inverse-transformed close values (original scale).\n",
    "    This builds a full-width array matching scaler.n_features_in_ and fills only close_idx column,\n",
    "    which avoids broadcasting shape errors.\n",
    "    \"\"\"\n",
    "    arr = np.asarray(scaled_close_array).reshape(-1)\n",
    "    n_features = scaler.n_features_in_ if hasattr(scaler, \"n_features_in_\") else len(FEATURES)\n",
    "    out = np.zeros((len(arr), n_features), dtype=float)\n",
    "    out[:, close_idx] = arr\n",
    "    inv = scaler.inverse_transform(out)\n",
    "    return inv[:, close_idx]\n",
    "\n",
    "# get scaled predictions and ground-truth\n",
    "y_test_pred_scaled = model.predict(X_test).flatten()\n",
    "y_test_inv = inverse_transform_close(y_test, scaler)            # y_test is scaled close values\n",
    "y_test_pred_inv = inverse_transform_close(y_test_pred_scaled, scaler)\n",
    "\n",
    "mae = mean_absolute_error(y_test_inv, y_test_pred_inv)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_inv, y_test_pred_inv))\n",
    "print(f\"Test MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 9) Feature importance check (permutation) - optional\n",
    "# -------------------------\n",
    "if 'sentiment' in FEATURES:\n",
    "    sent_idx = FEATURES.index('sentiment')\n",
    "    def permutation_importance_lstm(model, X, y_true, scaler, feature_idx, close_idx=CLOSE_IDX):\n",
    "        X_shuffled = X.copy()\n",
    "        n_samples, seq_len, n_features = X.shape\n",
    "        flat = X_shuffled[:,:,feature_idx].flatten()\n",
    "        np.random.shuffle(flat)\n",
    "        X_shuffled[:,:,feature_idx] = flat.reshape(n_samples, seq_len)\n",
    "\n",
    "        y_pred_scaled = model.predict(X_shuffled).flatten()\n",
    "        y_pred_inv = inverse_transform_close(y_pred_scaled, scaler)\n",
    "        y_true_inv = inverse_transform_close(y_true, scaler)\n",
    "\n",
    "        base_rmse = np.sqrt(mean_squared_error(y_true_inv, inverse_transform_close(model.predict(X).flatten(), scaler)))\n",
    "        new_rmse = np.sqrt(mean_squared_error(y_true_inv, y_pred_inv))\n",
    "\n",
    "        increase_pct = 100 * (new_rmse - base_rmse) / base_rmse if base_rmse > 0 else 0.0\n",
    "        return increase_pct\n",
    "\n",
    "    importance_sentiment = permutation_importance_lstm(model, X_test, y_test, scaler, sent_idx)\n",
    "    print(f\"Sentiment importance (RMSE increase % if shuffled): {importance_sentiment:.2f}%\")\n",
    "else:\n",
    "    importance_sentiment = None\n",
    "\n",
    "# Plot & save chunk of test actual vs predicted\n",
    "plot_n = min(200, len(y_test_inv))\n",
    "test_dates = df['Date'].iloc[seq_idx[test_mask]][-plot_n:]\n",
    "test_dates = pd.to_datetime(test_dates)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(test_dates, y_test_inv[-plot_n:], label='Actual Close')\n",
    "plt.plot(test_dates, y_test_pred_inv[-plot_n:], label='Predicted Close')\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))  # shows YYYY-MM\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "plt.xticks(rotation=30)\n",
    "plt.title('Test: Actual vs Predicted Close')\n",
    "plt.legend()\n",
    "plt.tight_layout(); plt.savefig(pred_plot_path); plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# 10) Classification: convert regression outputs -> UP/DOWN/UNCHANGED\n",
    "# -------------------------\n",
    "tol = 0.001\n",
    "\n",
    "test_indices = seq_idx[test_mask]\n",
    "prev_idx = np.maximum(test_indices - 1, 0)\n",
    "prev_close_vals = df[CLOSE_COL_NAME].iloc[prev_idx].values\n",
    "\n",
    "actual_rel = (y_test_inv - prev_close_vals) / (prev_close_vals + 1e-9)\n",
    "pred_rel = (y_test_pred_inv - prev_close_vals) / (prev_close_vals + 1e-9)\n",
    "\n",
    "def rel_to_label(rel_array, tol=tol):\n",
    "    labels = []\n",
    "    for r in np.asarray(rel_array).flatten():\n",
    "        if r > tol:\n",
    "            labels.append('UP')\n",
    "        elif r < -tol:\n",
    "            labels.append('DOWN')\n",
    "        else:\n",
    "            labels.append('UNCHANGED')\n",
    "    return np.array(labels)\n",
    "\n",
    "actual_labels = rel_to_label(actual_rel, tol=tol)\n",
    "pred_labels = rel_to_label(pred_rel, tol=tol)\n",
    "\n",
    "unique, counts = np.unique(actual_labels, return_counts=True)\n",
    "print(\"Actual label distribution:\", dict(zip(unique, counts)))\n",
    "unique, counts = np.unique(pred_labels, return_counts=True)\n",
    "print(\"Predicted label distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "acc = accuracy_score(actual_labels, pred_labels)\n",
    "prec_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(actual_labels, pred_labels, average='macro', zero_division=0)\n",
    "prec_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(actual_labels, pred_labels, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"\\n--- Direction classification metrics ---\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Macro Precision: {prec_macro:.4f}, Macro Recall: {recall_macro:.4f}, Macro F1: {f1_macro:.4f}\")\n",
    "print(f\"Weighted Precision: {prec_weighted:.4f}, Weighted Recall: {recall_weighted:.4f}, Weighted F1: {f1_weighted:.4f}\\n\")\n",
    "\n",
    "per_class_prec, per_class_rec, per_class_f1, _ = precision_recall_fscore_support(\n",
    "    actual_labels, pred_labels, labels=['UP','DOWN','UNCHANGED'], zero_division=0\n",
    ")\n",
    "print(\"Per-class (labels = ['UP','DOWN','UNCHANGED']):\")\n",
    "for lab, p, r, f in zip(['UP','DOWN','UNCHANGED'], per_class_prec, per_class_rec, per_class_f1):\n",
    "    print(f\"  {lab:9s} -> Precision: {p:.4f}, Recall: {r:.4f}, F1: {f:.4f}\")\n",
    "\n",
    "print(\"\\nFull classification report:\\n\")\n",
    "print(classification_report(actual_labels, pred_labels, labels=['UP','DOWN','UNCHANGED'], zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(actual_labels, pred_labels, labels=['UP','DOWN','UNCHANGED'])\n",
    "print(\"Confusion matrix (rows=actual, cols=predicted):\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=['UP','DOWN','UNCHANGED'], yticklabels=['UP','DOWN','UNCHANGED'])\n",
    "plt.xlabel('Predicted'); plt.ylabel('Actual'); plt.title('Confusion Matrix (direction)')\n",
    "plt.tight_layout(); plt.savefig(confusion_path); plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# 11) Predict next day\n",
    "# -------------------------\n",
    "last_seq = scaled_all[-SEQ_LEN:]\n",
    "assert last_seq.shape == (SEQ_LEN, len(FEATURES)), \"Last sequence shape mismatch\"\n",
    "\n",
    "last_seq = np.expand_dims(last_seq, axis=0)\n",
    "next_day_scaled = model.predict(last_seq).flatten()[0]\n",
    "next_day_price = inverse_transform_close([next_day_scaled], scaler)[0]\n",
    "\n",
    "last_close = df[CLOSE_COL_NAME].iloc[-1]\n",
    "movement = \"UP\" if next_day_price > last_close else (\"DOWN\" if next_day_price < last_close else \"UNCHANGED\")\n",
    "\n",
    "print(\"\\n--- Next day prediction ---\")\n",
    "print(f\"Last data date: {df['Date'].iloc[-1].date()}, last close: {last_close:.4f}\")\n",
    "print(f\"Predicted next-day close (approx): {next_day_price:.4f}\")\n",
    "print(f\"Predicted movement: {movement}\")\n",
    "\n",
    "# -------------------------\n",
    "# 12) Save outputs & model artifacts (symbol-named)\n",
    "# -------------------------\n",
    "out_df_reg = pd.DataFrame({\n",
    "    'Date': df['Date'].iloc[seq_idx[test_mask]].reset_index(drop=True),\n",
    "    'Actual_Close': y_test_inv,\n",
    "    'Pred_Close': y_test_pred_inv\n",
    "}).reset_index(drop=True)\n",
    "out_df_reg.to_csv(symbol_pred_csv, index=False)\n",
    "print(\"Saved:\", symbol_pred_csv)\n",
    "\n",
    "out_df = pd.DataFrame({\n",
    "    'Date': df['Date'].iloc[test_indices].reset_index(drop=True),\n",
    "    'Prev_Close': prev_close_vals,\n",
    "    'Actual_Close': y_test_inv,\n",
    "    'Pred_Close': y_test_pred_inv,\n",
    "    'Actual_RelChange': actual_rel,\n",
    "    'Pred_RelChange': pred_rel,\n",
    "    'Actual_Label': actual_labels,\n",
    "    'Pred_Label': pred_labels\n",
    "}).reset_index(drop=True)\n",
    "out_df.to_csv(symbol_pred_labels_csv, index=False)\n",
    "print(\"Saved:\", symbol_pred_labels_csv)\n",
    "\n",
    "last_seq_scaled_df = pd.DataFrame(scaled_all[-SEQ_LEN:], columns=FEATURES)\n",
    "last_seq_scaled_df.to_csv(symbol_last_seq_csv, index=False)\n",
    "print(\"Saved last sequence (scaled):\", symbol_last_seq_csv)\n",
    "\n",
    "joblib.dump(scaler, symbol_scaler_path)\n",
    "print(f\"Saved scaler to {symbol_scaler_path}\")\n",
    "\n",
    "try:\n",
    "    model.save(symbol_model_h5, include_optimizer=True)\n",
    "    print(f\"Saved HDF5 model to: {symbol_model_h5}\")\n",
    "except Exception as e:\n",
    "    print(\"Warning: saving HDF5 model failed:\", e)\n",
    "\n",
    "try:\n",
    "    model.save(symbol_savedmodel_dir, include_optimizer=True)\n",
    "    print(f\"Saved full SavedModel to {symbol_savedmodel_dir}\")\n",
    "except Exception as e:\n",
    "    print(\"Warning: saving SavedModel failed:\", e)\n",
    "\n",
    "meta = {\n",
    "    \"symbol\": symbol,\n",
    "    \"FEATURES\": FEATURES,\n",
    "    \"CLOSE_IDX\": CLOSE_IDX,\n",
    "    \"SEQ_LEN\": SEQ_LEN\n",
    "}\n",
    "with open(symbol_meta_json, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"Saved meta JSON:\", symbol_meta_json)\n",
    "\n",
    "# -------------------------\n",
    "# 13) Write consolidated JSON for DB ingestion\n",
    "# -------------------------\n",
    "def to_serializable(o):\n",
    "    if isinstance(o, (np.ndarray,)):\n",
    "        return o.tolist()\n",
    "    if isinstance(o, (np.float32, np.float64)):\n",
    "        return float(o)\n",
    "    if isinstance(o, (np.int32, np.int64)):\n",
    "        return int(o)\n",
    "    if isinstance(o, (pd.Timestamp,)):\n",
    "        return str(o)\n",
    "    return o\n",
    "\n",
    "pred_records = []\n",
    "test_dates_all = df['Date'].iloc[test_indices].dt.strftime('%Y-%m-%d').tolist()\n",
    "for i in range(len(y_test_inv)):\n",
    "    pred_records.append({\n",
    "        \"date\": test_dates_all[i],\n",
    "        \"prev_close\": float(prev_close_vals[i]),\n",
    "        \"actual_close\": float(y_test_inv[i]),\n",
    "        \"pred_close\": float(y_test_pred_inv[i]),\n",
    "        \"actual_relchange\": float(actual_rel[i]),\n",
    "        \"pred_relchange\": float(pred_rel[i]),\n",
    "        \"actual_label\": str(actual_labels[i]),\n",
    "        \"pred_label\": str(pred_labels[i])\n",
    "    })\n",
    "\n",
    "json_obj = {\n",
    "    \"symbol\": symbol,\n",
    "    \"generated_at_utc\": datetime.datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    \"n_rows\": int(n),\n",
    "    \"train_size\": int(train_end),\n",
    "    \"val_size\": int(val_within_train_n),\n",
    "    \"test_size\": int(len(y_test_inv)),\n",
    "    \"seq_len\": int(SEQ_LEN),\n",
    "    \"features\": FEATURES,\n",
    "    \"regression_metrics\": {\"mae\": float(mae), \"rmse\": float(rmse)},\n",
    "    \"classification_metrics\": {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"macro_precision\": float(prec_macro),\n",
    "        \"macro_recall\": float(recall_macro),\n",
    "        \"macro_f1\": float(f1_macro),\n",
    "        \"weighted_precision\": float(prec_weighted),\n",
    "        \"weighted_recall\": float(recall_weighted),\n",
    "        \"weighted_f1\": float(f1_weighted),\n",
    "        \"per_class\": {\n",
    "            \"UP\": {\"precision\": float(per_class_prec[0]), \"recall\": float(per_class_rec[0]), \"f1\": float(per_class_f1[0])},\n",
    "            \"DOWN\": {\"precision\": float(per_class_prec[1]), \"recall\": float(per_class_rec[1]), \"f1\": float(per_class_f1[1])},\n",
    "            \"UNCHANGED\": {\"precision\": float(per_class_prec[2]), \"recall\": float(per_class_rec[2]), \"f1\": float(per_class_f1[2])}\n",
    "        },\n",
    "        \"confusion_matrix\": cm.tolist()\n",
    "    },\n",
    "    \"next_day_prediction\": {\"pred_price\": float(next_day_price), \"pred_movement\": movement, \"last_close\": float(last_close), \"last_date\": str(df['Date'].iloc[-1].date())},\n",
    "    \"importance\": {\"sentiment_rmse_increase_pct\": float(importance_sentiment) if importance_sentiment is not None else None},\n",
    "    \"artifacts\": {\n",
    "        \"predictions_csv\": os.path.abspath(symbol_pred_csv),\n",
    "        \"predictions_with_labels_csv\": os.path.abspath(symbol_pred_labels_csv),\n",
    "        \"last_sequence_csv\": os.path.abspath(symbol_last_seq_csv),\n",
    "        \"scaler\": os.path.abspath(symbol_scaler_path),\n",
    "        \"model_h5\": os.path.abspath(symbol_model_h5) if os.path.exists(symbol_model_h5) else None,\n",
    "        \"saved_model_dir\": os.path.abspath(symbol_savedmodel_dir) if os.path.exists(symbol_savedmodel_dir) else None,\n",
    "        \"meta_json\": os.path.abspath(symbol_meta_json),\n",
    "        \"plot_prediction\": os.path.abspath(pred_plot_path),\n",
    "        \"plot_confusion\": os.path.abspath(confusion_path),\n",
    "        \"plot_train_loss\": os.path.abspath(loss_plot_path)\n",
    "    },\n",
    "    \"predictions\": pred_records\n",
    "}\n",
    "\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(json_obj, f, default=to_serializable, indent=2)\n",
    "print(\"Wrote consolidated JSON:\", json_path)\n",
    "\n",
    "print(f\"Model checkpoint (best) at: {ckpt_path}\")\n",
    "print(\"All done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf (py3.10)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
