{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "604dc0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training model for: NABIL\n",
      "============================================================\n",
      "\n",
      "Loading data...\n",
      "Loaded 3151 rows from 2012-01-01 to 2025-11-06\n",
      "\n",
      "Engineering features...\n",
      "After feature engineering: 3151 rows, 20 features\n",
      "Features: ['Close', 'Open', 'High', 'Low', 'Volume_norm', 'OBV_norm', 'Return', 'Return_lag1', 'Return_lag2', 'Return_lag5', 'EMA12', 'EMA26', 'SMA50', 'RSI', 'BB_position', 'MACD_hist', 'Volatility', 'ATR_norm', 'ROC', 'Price_position']\n",
      "\n",
      "Data splits:\n",
      "  Train: 2551 rows\n",
      "  Val:   284 rows\n",
      "  Test:  316 rows\n",
      "\n",
      "Creating sequences...\n",
      "Sequences - Train: 2491, Val: 284, Test: 316\n",
      "\n",
      "============================================================\n",
      "Building model (regularized to prevent overfitting)...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">14,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,720</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_12 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m14,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_13 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │         \u001b[38;5;34m9,720\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m31\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,951</span> (93.56 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,951\u001b[0m (93.56 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,951</span> (93.56 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,951\u001b[0m (93.56 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "Epoch 1/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 84ms/step - loss: 0.1561 - mae: 0.1940 - val_loss: 0.1116 - val_mae: 0.1679 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - loss: 0.0980 - mae: 0.1250 - val_loss: 0.0847 - val_mae: 0.1428 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 69ms/step - loss: 0.0781 - mae: 0.1149 - val_loss: 0.0796 - val_mae: 0.1702 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - loss: 0.0622 - mae: 0.1037 - val_loss: 0.0523 - val_mae: 0.1079 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 0.0514 - mae: 0.0955 - val_loss: 0.0488 - val_mae: 0.1228 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 0.0436 - mae: 0.0877 - val_loss: 0.0409 - val_mae: 0.1110 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 0.0381 - mae: 0.0843 - val_loss: 0.0357 - val_mae: 0.1068 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - loss: 0.0328 - mae: 0.0790 - val_loss: 0.0322 - val_mae: 0.1047 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 0.0301 - mae: 0.0783 - val_loss: 0.0259 - val_mae: 0.0852 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 0.0265 - mae: 0.0741 - val_loss: 0.0265 - val_mae: 0.1007 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 70ms/step - loss: 0.0247 - mae: 0.0735 - val_loss: 0.0196 - val_mae: 0.0704 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - loss: 0.0234 - mae: 0.0755 - val_loss: 0.0148 - val_mae: 0.0386 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 0.0215 - mae: 0.0735 - val_loss: 0.0125 - val_mae: 0.0294 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - loss: 0.0218 - mae: 0.0790 - val_loss: 0.0133 - val_mae: 0.0510 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - loss: 0.0193 - mae: 0.0725 - val_loss: 0.0114 - val_mae: 0.0397 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - loss: 0.0177 - mae: 0.0691 - val_loss: 0.0129 - val_mae: 0.0627 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 0.0162 - mae: 0.0666 - val_loss: 0.0103 - val_mae: 0.0460 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 0.0164 - mae: 0.0706 - val_loss: 0.0077 - val_mae: 0.0214 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - loss: 0.0164 - mae: 0.0733 - val_loss: 0.0073 - val_mae: 0.0227 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 0.0171 - mae: 0.0761 - val_loss: 0.0067 - val_mae: 0.0213 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 0.0158 - mae: 0.0738 - val_loss: 0.0062 - val_mae: 0.0185 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - loss: 0.0148 - mae: 0.0698 - val_loss: 0.0058 - val_mae: 0.0162 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - loss: 0.0148 - mae: 0.0714 - val_loss: 0.0059 - val_mae: 0.0286 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 0.0144 - mae: 0.0724 - val_loss: 0.0050 - val_mae: 0.0140 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 71ms/step - loss: 0.0134 - mae: 0.0696 - val_loss: 0.0054 - val_mae: 0.0282 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 71ms/step - loss: 0.0142 - mae: 0.0738 - val_loss: 0.0046 - val_mae: 0.0157 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 0.0131 - mae: 0.0704 - val_loss: 0.0043 - val_mae: 0.0156 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - loss: 0.0129 - mae: 0.0715 - val_loss: 0.0042 - val_mae: 0.0157 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 0.0120 - mae: 0.0683 - val_loss: 0.0047 - val_mae: 0.0268 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 0.0117 - mae: 0.0660 - val_loss: 0.0038 - val_mae: 0.0205 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 0.0120 - mae: 0.0682 - val_loss: 0.0043 - val_mae: 0.0316 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - loss: 0.0135 - mae: 0.0761 - val_loss: 0.0072 - val_mae: 0.0626 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - loss: 0.0136 - mae: 0.0779 - val_loss: 0.0038 - val_mae: 0.0260 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - loss: 0.0118 - mae: 0.0708 - val_loss: 0.0036 - val_mae: 0.0207 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 0.0105 - mae: 0.0652 - val_loss: 0.0038 - val_mae: 0.0298 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 0.0129 - mae: 0.0740 - val_loss: 0.0036 - val_mae: 0.0244 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 0.0112 - mae: 0.0688 - val_loss: 0.0036 - val_mae: 0.0268 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 76ms/step - loss: 0.0127 - mae: 0.0732 - val_loss: 0.0031 - val_mae: 0.0187 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - loss: 0.0132 - mae: 0.0769 - val_loss: 0.0029 - val_mae: 0.0150 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 0.0121 - mae: 0.0724 - val_loss: 0.0040 - val_mae: 0.0358 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 78ms/step - loss: 0.0117 - mae: 0.0717 - val_loss: 0.0047 - val_mae: 0.0424 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - loss: 0.0106 - mae: 0.0651 - val_loss: 0.0032 - val_mae: 0.0214 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 0.0104 - mae: 0.0655 - val_loss: 0.0031 - val_mae: 0.0231 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 0.0093 - mae: 0.0618 - val_loss: 0.0031 - val_mae: 0.0264 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 0.0115 - mae: 0.0730 - val_loss: 0.0028 - val_mae: 0.0243 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 0.0135 - mae: 0.0784 - val_loss: 0.0026 - val_mae: 0.0180 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - loss: 0.0151 - mae: 0.0830 - val_loss: 0.0039 - val_mae: 0.0377 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 0.0132 - mae: 0.0782 - val_loss: 0.0029 - val_mae: 0.0207 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - loss: 0.0113 - mae: 0.0697 - val_loss: 0.0043 - val_mae: 0.0428 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - loss: 0.0107 - mae: 0.0677 - val_loss: 0.0045 - val_mae: 0.0448 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - loss: 0.0105 - mae: 0.0675 - val_loss: 0.0027 - val_mae: 0.0199 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - loss: 0.0110 - mae: 0.0698 - val_loss: 0.0035 - val_mae: 0.0329 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0098 - mae: 0.0633\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 0.0095 - mae: 0.0631 - val_loss: 0.0030 - val_mae: 0.0264 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 0.0095 - mae: 0.0631 - val_loss: 0.0059 - val_mae: 0.0597 - learning_rate: 5.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 0.0094 - mae: 0.0624 - val_loss: 0.0045 - val_mae: 0.0466 - learning_rate: 5.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - loss: 0.0093 - mae: 0.0621 - val_loss: 0.0040 - val_mae: 0.0418 - learning_rate: 5.0000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - loss: 0.0087 - mae: 0.0605 - val_loss: 0.0067 - val_mae: 0.0668 - learning_rate: 5.0000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 0.0092 - mae: 0.0622 - val_loss: 0.0053 - val_mae: 0.0562 - learning_rate: 5.0000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 0.0092 - mae: 0.0622 - val_loss: 0.0055 - val_mae: 0.0573 - learning_rate: 5.0000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0094 - mae: 0.0632\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 0.0092 - mae: 0.0633 - val_loss: 0.0058 - val_mae: 0.0599 - learning_rate: 5.0000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 78ms/step - loss: 0.0085 - mae: 0.0593 - val_loss: 0.0053 - val_mae: 0.0569 - learning_rate: 2.5000e-04\n",
      "Epoch 61: early stopping\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "\n",
      "============================================================\n",
      "Evaluating on test set...\n",
      "============================================================\n",
      "\n",
      "Regression Metrics:\n",
      "  MAE:  16.52\n",
      "  RMSE: 23.90\n",
      "  MAPE: 3.03%\n",
      "  R²:   0.7344\n",
      "\n",
      "============================================================\n",
      "Direction Classification...\n",
      "============================================================\n",
      "Volatility: 0.0109\n",
      "Tolerance:  0.0033 (0.33%)\n",
      "\n",
      "Actual distribution: {np.str_('DOWN'): np.int64(182), np.str_('UP'): np.int64(134)}\n",
      "Predicted distribution: {np.str_('DOWN'): np.int64(221), np.str_('UP'): np.int64(95)}\n",
      "\n",
      "Classification Accuracy: 0.5728 (57.28%)\n",
      "Weighted Precision: 0.5590\n",
      "Weighted Recall:    0.5728\n",
      "Weighted F1:        0.5571\n",
      "\n",
      "Per-class metrics:\n",
      "  UP         -> Prec: 0.495, Rec: 0.351, F1: 0.410, Support: 134\n",
      "  DOWN       -> Prec: 0.606, Rec: 0.736, F1: 0.665, Support: 182\n",
      "\n",
      "============================================================\n",
      "Next Day Prediction...\n",
      "============================================================\n",
      "\n",
      "Last Date:       2025-11-06\n",
      "Last Close:      507.30\n",
      "Predicted Date:  2025-11-07\n",
      "Predicted Close: 501.40\n",
      "Change:          -5.90 (-1.16%)\n",
      "Direction:       DOWN\n",
      "\n",
      "============================================================\n",
      "Saving results...\n",
      "============================================================\n",
      "✓ Saved: NABIL_results.csv\n",
      "✓ Saved: NABIL_model.keras\n",
      "✓ Saved: NABIL_scaler.pkl\n",
      "✓ Saved: NABIL_results.json\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Regression  - MAE: 16.52, RMSE: 23.90, R²: 0.734\n",
      "Classification - Accuracy: 57.28%, F1: 0.557\n",
      "Next Day    - 507.30 → 501.40 (-1.16%)\n",
      "============================================================\n",
      "\n",
      "All outputs saved to: outputs\\NABIL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "DATA_PATH = \"NABIL.csv\"\n",
    "SEQ_LEN = 60\n",
    "\n",
    "TEST_PCT = 0.10  # Increased for better evaluation\n",
    "VAL_WITHIN_TRAIN_PCT = 0.10\n",
    "\n",
    "# CRITICAL FIX: Use binary classification (removes massive UNCHANGED class imbalance)\n",
    "USE_BINARY_CLASSIFICATION = True\n",
    "\n",
    "# CRITICAL FIX: Adaptive tolerance based on recent volatility\n",
    "TOLERANCE_MULTIPLIER = 0.3  # Lower = more conservative\n",
    "\n",
    "CLOSE_COL_NAME = 'Close'\n",
    "\n",
    "symbol = os.path.splitext(os.path.basename(DATA_PATH))[0].upper()\n",
    "OUTPUTS_DIR = \"outputs\"\n",
    "symbol_dir = os.path.join(OUTPUTS_DIR, symbol)\n",
    "os.makedirs(symbol_dir, exist_ok=True)\n",
    "\n",
    "print(f\"=\" * 60)\n",
    "print(f\"Training model for: {symbol}\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# -------------------------\n",
    "# Carefully Selected Features (avoid multicollinearity)\n",
    "# -------------------------\n",
    "def compute_selected_indicators(df):\n",
    "    \"\"\"Only non-redundant, proven indicators\"\"\"\n",
    "    \n",
    "    # Price features\n",
    "    df['Return'] = df['Close'].pct_change().fillna(0)\n",
    "    \n",
    "    # Trend indicators (avoid redundancy - pick best of each type)\n",
    "    df['EMA12'] = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    df['EMA26'] = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['SMA50'] = df['Close'].rolling(50, min_periods=1).mean()\n",
    "    \n",
    "    # Momentum\n",
    "    delta = df['Close'].diff()\n",
    "    gain = delta.clip(lower=0).rolling(14, min_periods=1).mean()\n",
    "    loss = -delta.clip(upper=0).rolling(14, min_periods=1).mean()\n",
    "    rs = gain / (loss + 1e-9)\n",
    "    df['RSI'] = (100 - (100 / (1 + rs))).fillna(50)\n",
    "    \n",
    "    # Volatility\n",
    "    df['Volatility'] = df['Close'].pct_change().rolling(20, min_periods=1).std().fillna(0)\n",
    "    \n",
    "    # MACD\n",
    "    df['MACD'] = df['EMA12'] - df['EMA26']\n",
    "    df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    df['MACD_hist'] = df['MACD'] - df['MACD_signal']\n",
    "    \n",
    "    # Bollinger position (normalized)\n",
    "    sma20 = df['Close'].rolling(20, min_periods=1).mean()\n",
    "    std20 = df['Close'].rolling(20, min_periods=1).std()\n",
    "    bb_upper = sma20 + 2 * std20\n",
    "    bb_lower = sma20 - 2 * std20\n",
    "    df['BB_position'] = ((df['Close'] - bb_lower) / (bb_upper - bb_lower + 1e-9)).fillna(0.5).clip(0, 1)\n",
    "    \n",
    "    # ATR (normalized)\n",
    "    if 'High' in df.columns and 'Low' in df.columns:\n",
    "        tr = df['High'] - df['Low']\n",
    "        atr = tr.rolling(14, min_periods=1).mean()\n",
    "        df['ATR_norm'] = (atr / df['Close']).fillna(0)\n",
    "    else:\n",
    "        df['ATR_norm'] = df['Volatility']\n",
    "    \n",
    "    # Volume indicators\n",
    "    if 'Volume' in df.columns:\n",
    "        df['Volume_norm'] = df['Volume'] / df['Volume'].rolling(20, min_periods=1).mean()\n",
    "        df['Volume_norm'] = df['Volume_norm'].fillna(1).clip(0, 5)  # Cap extreme values\n",
    "        \n",
    "        # OBV (On Balance Volume) - normalized\n",
    "        obv = (np.sign(df['Close'].diff()) * df['Volume']).fillna(0).cumsum()\n",
    "        df['OBV_norm'] = (obv - obv.rolling(50, min_periods=1).mean()) / (obv.rolling(50, min_periods=1).std() + 1e-9)\n",
    "        df['OBV_norm'] = df['OBV_norm'].fillna(0).clip(-3, 3)\n",
    "    else:\n",
    "        df['Volume_norm'] = 1\n",
    "        df['OBV_norm'] = 0\n",
    "    \n",
    "    # Rate of Change\n",
    "    df['ROC'] = df['Close'].pct_change(12).fillna(0) * 100\n",
    "    df['ROC'] = df['ROC'].clip(-20, 20)  # Cap extreme values\n",
    "    \n",
    "    # Lagged returns (most predictive)\n",
    "    df['Return_lag1'] = df['Return'].shift(1).fillna(0)\n",
    "    df['Return_lag2'] = df['Return'].shift(2).fillna(0)\n",
    "    df['Return_lag5'] = df['Return'].shift(5).fillna(0)\n",
    "    \n",
    "    # Price position in range\n",
    "    roll_min = df['Close'].rolling(20, min_periods=1).min()\n",
    "    roll_max = df['Close'].rolling(20, min_periods=1).max()\n",
    "    df['Price_position'] = ((df['Close'] - roll_min) / (roll_max - roll_min + 1e-9)).fillna(0.5)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# -------------------------\n",
    "# Load and Clean Data\n",
    "# -------------------------\n",
    "print(\"\\nLoading data...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = [c.strip().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "# Clean numeric columns\n",
    "for col in ['Open', 'High', 'Low', 'Close', 'Volume', 'Turnover']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '').str.replace('\"', ''), errors='coerce')\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded {len(df)} rows from {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
    "\n",
    "# Handle outliers (winsorization)\n",
    "for col in ['Close', 'Volume']:\n",
    "    if col in df.columns and df[col].notna().sum() > 0:\n",
    "        q01, q99 = df[col].quantile([0.01, 0.99])\n",
    "        df[col] = df[col].clip(q01, q99)\n",
    "\n",
    "# -------------------------\n",
    "# Feature Engineering\n",
    "# -------------------------\n",
    "print(\"\\nEngineering features...\")\n",
    "df = compute_selected_indicators(df)\n",
    "\n",
    "# CAREFULLY SELECTED FEATURES (no redundancy)\n",
    "FEATURES = [\n",
    "    'Close', 'Open', 'High', 'Low',  # Price\n",
    "    'Volume_norm', 'OBV_norm',  # Volume\n",
    "    'Return', 'Return_lag1', 'Return_lag2', 'Return_lag5',  # Momentum\n",
    "    'EMA12', 'EMA26', 'SMA50',  # Trend (no overlap)\n",
    "    'RSI', 'BB_position',  # Oscillators\n",
    "    'MACD_hist',  # MACD (just histogram, most informative)\n",
    "    'Volatility', 'ATR_norm',  # Volatility\n",
    "    'ROC', 'Price_position'  # Additional momentum\n",
    "]\n",
    "\n",
    "# Filter to available\n",
    "FEATURES = [f for f in FEATURES if f in df.columns]\n",
    "CLOSE_IDX = FEATURES.index('Close')\n",
    "\n",
    "# Remove rows with NaN\n",
    "df = df[FEATURES + ['Date']].dropna().reset_index(drop=True)\n",
    "print(f\"After feature engineering: {len(df)} rows, {len(FEATURES)} features\")\n",
    "print(f\"Features: {FEATURES}\")\n",
    "\n",
    "# -------------------------\n",
    "# Split Data\n",
    "# -------------------------\n",
    "n = len(df)\n",
    "if n < 500:\n",
    "    raise ValueError(\"Need at least 500 rows of data for reliable training\")\n",
    "\n",
    "test_start = int(n * (1 - TEST_PCT))\n",
    "train_end = int(test_start * (1 - VAL_WITHIN_TRAIN_PCT))\n",
    "val_start = train_end\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"  Train: {train_end} rows\")\n",
    "print(f\"  Val:   {test_start - train_end} rows\")\n",
    "print(f\"  Test:  {n - test_start} rows\")\n",
    "\n",
    "# Convert to float32 and scale\n",
    "raw_data = df[FEATURES].values.astype(np.float32)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(raw_data[:train_end])\n",
    "scaled_all = scaler.transform(raw_data)\n",
    "\n",
    "# -------------------------\n",
    "# Create Sequences\n",
    "# -------------------------\n",
    "print(\"\\nCreating sequences...\")\n",
    "\n",
    "def create_sequences(data, seq_len, close_idx):\n",
    "    X, y, idx = [], [], []\n",
    "    for i in range(seq_len, len(data)):\n",
    "        X.append(data[i-seq_len:i])\n",
    "        y.append(data[i, close_idx])\n",
    "        idx.append(i)\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32), np.array(idx)\n",
    "\n",
    "X_all, y_all, seq_idx = create_sequences(scaled_all, SEQ_LEN, CLOSE_IDX)\n",
    "\n",
    "train_mask = seq_idx < train_end\n",
    "val_mask = (seq_idx >= val_start) & (seq_idx < test_start)\n",
    "test_mask = seq_idx >= test_start\n",
    "\n",
    "X_train, y_train = X_all[train_mask], y_all[train_mask]\n",
    "X_val, y_val = X_all[val_mask], y_all[val_mask]\n",
    "X_test, y_test = X_all[test_mask], y_all[test_mask]\n",
    "\n",
    "print(f\"Sequences - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Build Model (SIMPLE to avoid overfitting)\n",
    "# -------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Building model (regularized to prevent overfitting)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, \n",
    "         recurrent_dropout=0.2,\n",
    "         kernel_regularizer=regularizers.l2(0.001),\n",
    "         input_shape=(SEQ_LEN, len(FEATURES))),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    LSTM(30, recurrent_dropout=0.2,\n",
    "         kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss=\"mse\", metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# Train\n",
    "# -------------------------\n",
    "print(\"\\nTraining...\")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, verbose=1, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(symbol_dir, f\"{symbol}_training.png\"), dpi=100)\n",
    "plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Evaluate\n",
    "# -------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluating on test set...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def inverse_transform_close(scaled_vals, scaler, close_idx):\n",
    "    \"\"\"Inverse transform close prices\"\"\"\n",
    "    arr = np.asarray(scaled_vals).flatten()\n",
    "    out = np.zeros((len(arr), len(FEATURES)), dtype=np.float32)\n",
    "    out[:, close_idx] = arr\n",
    "    return scaler.inverse_transform(out)[:, close_idx]\n",
    "\n",
    "y_test_pred_scaled = model.predict(X_test, batch_size=64, verbose=0).flatten()\n",
    "\n",
    "y_test_actual = inverse_transform_close(y_test, scaler, CLOSE_IDX)\n",
    "y_test_pred = inverse_transform_close(y_test_pred_scaled, scaler, CLOSE_IDX)\n",
    "\n",
    "# Regression metrics\n",
    "mae = mean_absolute_error(y_test_actual, y_test_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_actual, y_test_pred))\n",
    "mape = np.mean(np.abs((y_test_actual - y_test_pred) / (y_test_actual + 1e-9))) * 100\n",
    "r2 = r2_score(y_test_actual, y_test_pred)\n",
    "\n",
    "print(f\"\\nRegression Metrics:\")\n",
    "print(f\"  MAE:  {mae:.2f}\")\n",
    "print(f\"  RMSE: {rmse:.2f}\")\n",
    "print(f\"  MAPE: {mape:.2f}%\")\n",
    "print(f\"  R²:   {r2:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(14,5))\n",
    "plot_n = min(200, len(y_test_actual))\n",
    "x_range = range(plot_n)\n",
    "\n",
    "plt.plot(x_range, y_test_actual[-plot_n:], label='Actual', linewidth=2, alpha=0.8)\n",
    "plt.plot(x_range, y_test_pred[-plot_n:], label='Predicted', linewidth=2, alpha=0.8)\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title(f'Test Predictions (MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.3f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(symbol_dir, f\"{symbol}_predictions.png\"), dpi=100)\n",
    "plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Classification (FIXED with binary and adaptive tolerance)\n",
    "# -------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Direction Classification...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_indices = seq_idx[test_mask]\n",
    "prev_close = df['Close'].iloc[np.maximum(test_indices - 1, 0)].values\n",
    "\n",
    "actual_change = (y_test_actual - prev_close) / (prev_close + 1e-9)\n",
    "pred_change = (y_test_pred - prev_close) / (prev_close + 1e-9)\n",
    "\n",
    "# Calculate adaptive tolerance from recent volatility\n",
    "recent_volatility = df['Close'].iloc[-200:].pct_change().std()\n",
    "tolerance = recent_volatility * TOLERANCE_MULTIPLIER\n",
    "\n",
    "print(f\"Volatility: {recent_volatility:.4f}\")\n",
    "print(f\"Tolerance:  {tolerance:.4f} ({tolerance*100:.2f}%)\")\n",
    "\n",
    "def classify_direction(changes, tol, binary=True):\n",
    "    \"\"\"Classify price movements\"\"\"\n",
    "    if binary:\n",
    "        # Binary: UP if change >= 0, DOWN otherwise\n",
    "        return np.where(changes >= 0, 'UP', 'DOWN')\n",
    "    else:\n",
    "        # Ternary with tolerance\n",
    "        return np.where(changes > tol, 'UP',\n",
    "                       np.where(changes < -tol, 'DOWN', 'UNCHANGED'))\n",
    "\n",
    "actual_labels = classify_direction(actual_change, tolerance, USE_BINARY_CLASSIFICATION)\n",
    "pred_labels = classify_direction(pred_change, tolerance, USE_BINARY_CLASSIFICATION)\n",
    "\n",
    "# Count distribution\n",
    "unique_actual, counts_actual = np.unique(actual_labels, return_counts=True)\n",
    "unique_pred, counts_pred = np.unique(pred_labels, return_counts=True)\n",
    "\n",
    "print(f\"\\nActual distribution: {dict(zip(unique_actual, counts_actual))}\")\n",
    "print(f\"Predicted distribution: {dict(zip(unique_pred, counts_pred))}\")\n",
    "\n",
    "# Metrics\n",
    "labels_list = ['UP', 'DOWN'] if USE_BINARY_CLASSIFICATION else ['UP', 'DOWN', 'UNCHANGED']\n",
    "acc = accuracy_score(actual_labels, pred_labels)\n",
    "\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    actual_labels, pred_labels, labels=labels_list, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nClassification Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "print(f\"Weighted Precision: {prec:.4f}\")\n",
    "print(f\"Weighted Recall:    {rec:.4f}\")\n",
    "print(f\"Weighted F1:        {f1:.4f}\")\n",
    "\n",
    "# Per-class\n",
    "prec_c, rec_c, f1_c, sup_c = precision_recall_fscore_support(\n",
    "    actual_labels, pred_labels, labels=labels_list, zero_division=0)\n",
    "\n",
    "print(f\"\\nPer-class metrics:\")\n",
    "for i, label in enumerate(labels_list):\n",
    "    print(f\"  {label:10s} -> Prec: {prec_c[i]:.3f}, Rec: {rec_c[i]:.3f}, F1: {f1_c[i]:.3f}, Support: {sup_c[i]}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(actual_labels, pred_labels, labels=labels_list)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', \n",
    "            xticklabels=labels_list, yticklabels=labels_list,\n",
    "            cmap='Blues', cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix\\nAccuracy: {acc:.2%}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(symbol_dir, f\"{symbol}_confusion.png\"), dpi=100)\n",
    "plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Next Day Prediction\n",
    "# -------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Next Day Prediction...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "last_seq = scaled_all[-SEQ_LEN:].reshape(1, SEQ_LEN, -1)\n",
    "next_pred_scaled = model.predict(last_seq, verbose=0)[0, 0]\n",
    "next_pred_price = inverse_transform_close([next_pred_scaled], scaler, CLOSE_IDX)[0]\n",
    "\n",
    "last_close = df['Close'].iloc[-1]\n",
    "last_date = df['Date'].iloc[-1]\n",
    "next_date = last_date + pd.Timedelta(days=1)\n",
    "\n",
    "change_amount = next_pred_price - last_close\n",
    "change_pct = (change_amount / last_close) * 100\n",
    "direction = 'UP' if change_amount > 0 else 'DOWN'\n",
    "\n",
    "print(f\"\\nLast Date:       {last_date.date()}\")\n",
    "print(f\"Last Close:      {last_close:.2f}\")\n",
    "print(f\"Predicted Date:  {next_date.date()}\")\n",
    "print(f\"Predicted Close: {next_pred_price:.2f}\")\n",
    "print(f\"Change:          {change_amount:+.2f} ({change_pct:+.2f}%)\")\n",
    "print(f\"Direction:       {direction}\")\n",
    "\n",
    "# Sanity check\n",
    "if abs(change_pct) > 10:\n",
    "    print(f\"\\n WARNING: Predicted change of {change_pct:+.2f}% seems extreme!\")\n",
    "    print(\"   Model may need more training data or tuning.\")\n",
    "\n",
    "# -------------------------\n",
    "# Save Results\n",
    "# -------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Saving results...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# CSV with predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'Date': df['Date'].iloc[test_indices].reset_index(drop=True),\n",
    "    'Actual_Close': y_test_actual,\n",
    "    'Predicted_Close': y_test_pred,\n",
    "    'Error': y_test_actual - y_test_pred,\n",
    "    'Error_Pct': ((y_test_actual - y_test_pred) / y_test_actual) * 100,\n",
    "    'Actual_Direction': actual_labels,\n",
    "    'Predicted_Direction': pred_labels,\n",
    "    'Correct': actual_labels == pred_labels\n",
    "})\n",
    "results_df.to_csv(os.path.join(symbol_dir, f\"{symbol}_results.csv\"), index=False)\n",
    "print(f\"✓ Saved: {symbol}_results.csv\")\n",
    "\n",
    "# Save model and scaler\n",
    "model.save(os.path.join(symbol_dir, f\"{symbol}_model.keras\"))\n",
    "joblib.dump(scaler, os.path.join(symbol_dir, f\"{symbol}_scaler.pkl\"))\n",
    "print(f\"✓ Saved: {symbol}_model.keras\")\n",
    "print(f\"✓ Saved: {symbol}_scaler.pkl\")\n",
    "\n",
    "# Save configuration and results\n",
    "results_json = {\n",
    "    \"symbol\": symbol,\n",
    "    \"timestamp\": datetime.datetime.utcnow().isoformat(),\n",
    "    \"config\": {\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"features\": FEATURES,\n",
    "        \"num_features\": len(FEATURES),\n",
    "        \"binary_classification\": USE_BINARY_CLASSIFICATION,\n",
    "        \"tolerance\": float(tolerance),\n",
    "        \"test_pct\": TEST_PCT\n",
    "    },\n",
    "    \"regression\": {\n",
    "        \"mae\": float(mae),\n",
    "        \"rmse\": float(rmse),\n",
    "        \"mape\": float(mape),\n",
    "        \"r2\": float(r2)\n",
    "    },\n",
    "    \"classification\": {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"weighted_precision\": float(prec),\n",
    "        \"weighted_recall\": float(rec),\n",
    "        \"weighted_f1\": float(f1)\n",
    "    },\n",
    "    \"next_prediction\": {\n",
    "        \"last_date\": str(last_date.date()),\n",
    "        \"last_close\": float(last_close),\n",
    "        \"predicted_date\": str(next_date.date()),\n",
    "        \"predicted_close\": float(next_pred_price),\n",
    "        \"change_amount\": float(change_amount),\n",
    "        \"change_pct\": float(change_pct),\n",
    "        \"direction\": direction\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(symbol_dir, f\"{symbol}_results.json\"), \"w\") as f:\n",
    "    json.dump(results_json, f, indent=2)\n",
    "print(f\"✓ Saved: {symbol}_results.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Regression  - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.3f}\")\n",
    "print(f\"Classification - Accuracy: {acc:.2%}, F1: {f1:.3f}\")\n",
    "print(f\"Next Day    - {last_close:.2f} → {next_pred_price:.2f} ({change_pct:+.2f}%)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAll outputs saved to: {symbol_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf (py3.10)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
